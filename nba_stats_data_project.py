# -*- coding: utf-8 -*-
"""NBA STATS Data Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oJyDFSMB6_-X0UNCe9EejxXRsGwXqgHc

# DE 322 Final Project
## Player stats and information for NBA players during the 2015-2016 season

## Project Question?
### Based on niche player facts and information such as college affiliation, draft position, and jersey number, what players during the 2015-2016 NBA season performed the best in a variety of statistical categories.

### Need to extract data on game by game play logs for the 2015-2016 NBA season as well as information on every NBA player such as college, draft position, jersey number, etc.

##Roadmap:
##### 1) Extract game by game statistics from CSV file, clean and load into DF.
##### 2) Extract NBA player information from API, load into dataframe, clean and load into cleaned DF.
##### 3) Merge cleaned dataframes
##### 4) Transform data to get player info categories with the highest and lowest statistical performers.
##### 5) Load data into SQL database.

### Measure: Multiple different numerical columns such as PTS, REB, AST that can be sorted

# Extract

Data is sourced from an API retrieved via a public API named "balldontlie" and a csv file sourced from github.

1) Github - https://github.com/mvanbommel/scrape_nba_box_scores/blob/master/2015_2016_NBA_box_score_player_data.csv

2) BallDontLie API - https://docs.balldontlie.io/#get-a-specific-player
"""

# Package Imports

import pandas as pd
import numpy as np
import requests
from google.colab import files

# NBA game by game statistics
game_by_game_stats = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vRKOmKH7eiA1T_XUqP0DSmL8HaSS6L_OGVN8SoV9uJI0fUyLbcpDoEqqoT5QzK3pEOZKTVbVnQcNCz7/pub?output=csv")

# Explore data
# The data contains 31,454 rows of game data and 27 columns of information and stats
# I am focused on just the player column and the statistical categories
game_by_game_stats

"""# API call starts here. You do not need to run these cells as it will take almost an hour to do so, but you can start the cell to see that it functions

# This cell breaks down how I pulled the nba player info from the API
# There are over 5,000 rows and with 5 calls per minute due to the API restrictions this took almost an hour
# You do not need to rerun this cell as I stored the API into a CSV and imported it
!pip install balldontlie
from balldontlie import BalldontlieAPI
import json
from pandas import json_normalize
import time # Added import for time module
api_key = "62fe032c-e027-45b9-8402-9a9e87ea23fb"
api = BalldontlieAPI(api_key)

all_players = [] #The API will append to this list which will be turned into a DF

# This API has 20 columns and 5526 rows so the import was challenging.
# Was limited to 100 players per page due to paywall

cursor = None # This will be used to let us know when we have reached the end of the data

while True:
  response = api.nba.players.list(per_page=100, cursor=cursor)

  all_players.extend([player.model_dump() for player in response.data]) # Takes the players in the current page and adds to list

  cursor = response.meta.next_cursor

  if cursor is None:
    break #Signals we are at the end of the data

  time.sleep(30) # Included to avoid rate limiting so our loop will not time out

all_players_df = pd.json_normalize(all_players) # Converts players into DF

all_players_df.drop_duplicates(subset=['id'], inplace=True) # Included this because I had to rerun a few times to get it to work and there were duplicates.

# I saved this data to a CSV so I had easier access to the dataset.
player_data = all_players_df

player_data.to_csv('player_data_api.csv', index=False)

files.download('player_data_api.csv')

# API call ends here.
"""

# Here is the CSV file loaded into the notebook

player_info = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vSkIlttlfmQ5HE9ymRCF3qe_5Spg6-BIOKNSqs_HCBQWacRkFnzqKlVuhz8jH3ReeKLnb6cAiJGJLbK/pub?output=csv")

# Explore Data
# There is quite a bit here, but I am looking for player name so I can make a new column
# I am also looking for niche facts to analyze such as college, country of origin, draft position etc
# There are 5526 rows and 20 columns
player_info

"""# Transform

The game by game DF will need to be changed into a per player averages.

I will need to drop unneccesary columns, fill or drop NA values to make sense of statistical data, convert statistical columns into numeric columns so they can be operated on, group by player and team to get averages and then aggregate on the statistical columns.

# Game DF will be transformed first
"""

# Dropping unnecessary columns
# Will make a copy of DF as a safety net
game_df = game_by_game_stats.copy()

game_df = game_df.drop(columns=['Unnamed: 0', 'GameID', 'Opponent', 'Home', 'Win', 'Starter', 'DNP'])

# Checking NA values
game_df.isna().sum()

# Some columns have '--' in place of no stat being recorded. I will replace that with NA
game_df = game_df.replace('--', np.nan)

game_df.info() # I will make numeric columns into quantitative data

# Column conversion to numeric columns. Any non numeric value is converted to a NaN value
columns_to_convert = ['MIN', 'FGM', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PM', 'PTS']
for col in columns_to_convert:
  game_df[col] = pd.to_numeric(game_df[col], errors='coerce')

# The data is grouped by team and player to join like players together across the DF.
# Now showing 536 rows
game_df = game_df.groupby(["Player", "Team"])[['PTS','REB','AST', 'TO', 'PF', 'STL', 'BLK', 'MIN', 'FGA', 'FGM', '3:00 PM', '3PA', 'FTM', 'FTA', 'PM']].agg(['mean']).round(1)

# Will need to flatten multi-level columns and reset index to make player and team regular columns
game_df.columns = ['_'.join(col).strip() for col in game_df.columns.values]

# Reset Index
game_df = game_df.reset_index()

game_df.head() # Check new DF

game_df['Player'] = game_df['Player'].str.strip() # Removing whitespace for formatting purposes

game_df.head()

# There are duplicate players in the data. Some were traded midseason so have multiple teams.
# Some have similar names. Same first initial and last name so that will be addressed.
print(game_df['Player'].nunique())
print(game_df.shape)

"""Using google I went through the duplicates to verify if they were indeed duplicates or different players entirely. The list below are all of the players with similar names but different people."""

# This cell pulls up a DF of all duplicate players in the dataset
duplicate_player_rows = game_df[game_df['Player'].duplicated(keep=False)]
duplicate_player_rows

game_df.info()

# I used integer location to manually change the name of the players with similar names
game_df.iloc[455, 0] = "Se. Curry"
game_df.iloc[119, 0] = "Dr. Green"
game_df.iloc[143, 0] = "Dk. Williams"
game_df.iloc[146, 0] = "Dl. Wright"
game_df.iloc[195, 0] = "Ju. Anderson"
game_df.iloc[218, 0] = "Jn. Grant"
game_df.iloc[220, 0] = "Jm. Green"
game_df.iloc[232, 0] = "Jr. Holiday"
game_df.iloc[241, 0] = "Ja. Johnson"
game_df.iloc[268, 0] = "Ja. Smith"
game_df.iloc[365, 0] = "Ma. Morris"
game_df.iloc[369, 0] = "Ml. Plumlee"
game_df.iloc[380, 0] = "Ma. Williams"

game_df.iloc[119] # This cell is to check if the previous was successful

# The game_df dataset has two rows that need to be removed as they both have no player name
indices_to_drop = game_df.iloc[[0, 535]].index.to_list()
game_df = game_df.drop(index=indices_to_drop)
game_df

# I will now rename and drop columns as well as adding percentages to make the data look more professional
# I will rename columns to finish up this dataframe and make it look presentable
new_columns = []
for col in game_df.columns:
    if col.endswith('_mean'):
        new_columns.append(col.replace('_mean', ''))
    else:
        new_columns.append(col)

game_df.columns = new_columns
# My 3PM column was read in as a time so that will need to be changed as well
game_df = game_df.rename(columns={"3:00 PM": "3PM"})
game_df

game_df.head()

# Now to add shooting percentages
game_df["3PT %"] = round((game_df['3PM'] / game_df['3PA']) * 100, 1)
game_df["FG %"] = round((game_df['FGM'] / game_df['FGA']) * 100, 1)
game_df["FT %"] = round((game_df['FTM'] / game_df['FTA']) * 100, 1)

game_df.loc[game_df['Player'] == 'Se. Curry'] # To verify the Player name change was successful

# Now that duplicates have been addressed they will be combined and the team column will be dropped.
game_df = game_df.groupby("Player")[['PTS','REB','AST', 'TO', 'PF', 'STL', 'BLK', 'MIN', 'FGA', 'FGM', '3PM', '3PA', 'FTM', 'FTA', 'PM', '3PT %', 'FG %', 'FT %']].agg(['mean']).round(1).reset_index()

# My columns will need to be renamed to remove mean
game_df = game_df.rename(columns=lambda column_name: column_name.replace('mean', ''))

# I need to drop level so I can merge later on
game_df.columns = game_df.columns.droplevel(1)

game_df # Now each player has their own unique row

"""# Player Info will now be transformed"""

player_info.info() # There are 5526 rows.

# The draft data is a float but I will change it to an int so I can remove the trailing 0
player_info[['draft_year', 'draft_round', 'draft_number']] = player_info[['draft_year', 'draft_round', 'draft_number']].astype('Int64')

player_info

# Now I will drop some columns that I will not be needing such as team info
player_info.columns

player_info = player_info.drop(columns=['id', 'team_id', 'team.id', 'team.conference', 'team.division', 'team.city', 'team.name', 'team.full_name', 'team.abbreviation'])

# Before I rename columns I need to make a player column to join on my grouped dataset

player_info['Player'] = player_info['first_name'].str[0:1] + ". " + player_info['last_name']
player_info.head() #Inspect

# Renaming Columns
print(player_info.columns)
player_info = player_info.rename(columns={
    "position": "Position",
    "height": "Height",
    "weight": "Weight",
    "jersey_number": "Jersey Num",
    "college": "College",
    "country": "Country",
    "draft_year": "Draft Yr",
    "draft_round": "Draft Round",
    "draft_number": "Draft Number"

})
player_info.head()

player_info.loc[player_info['Player'] == 'M. Williams'] # Checking data

# This dataset contains the same players with similar names.
# I will only hardcode the names from earlier as I will do a left join on the data
duplicate_player_info = player_info[player_info['Player'].duplicated(keep=False)]
duplicate_player_info.sort_values(by="Player")

# Same list from previous dataset, hardcoded the same.
player_info.iloc[113, -1] = "Se. Curry"
player_info.iloc[184, -1] = "Dr. Green"
player_info.iloc[1940, -1] = "Dk. Williams"
player_info.iloc[486, -1] = "Dl. Wright"
player_info.iloc[10, -1] = "Ju. Anderson"
player_info.iloc[182, -1] = "Jn. Grant"
player_info.iloc[186, -1] = "Jm. Green"
player_info.iloc[213, -1] = "Jr. Holiday"
player_info.iloc[241, -1] = "Ja. Johnson"
player_info.iloc[422, -1] = "Ja. Smith"
player_info.iloc[327, -1] = "Ma. Morris"
player_info.iloc[2178, -1] = "Ml. Plumlee"
player_info.iloc[481, -1] = "Ma. Williams"

player_info.loc[player_info['Player'] == 'Se. Curry'] # Checking to see transformation were successful

# Lastly I will drop the first and last name column and move my Player column to the front
player_info = player_info.drop(['first_name', 'last_name'], axis=1)

# Column position change
new_order = ['Player', 'Position', 'Height', 'Weight', 'Jersey Num', 'College', 'Country', 'Draft Yr', 'Draft Round', 'Draft Number']
player_info = player_info[new_order]

player_info.head()

"""# Merge

Now that both datasets are cleaned I will merge them on the Player column
"""

player_info.head()

game_df.head()

# Merged dataframe on left join
merged_player_data = game_df.merge(
    player_info,
    left_on='Player',
    right_on="Player",
    how='left'
)

merged_player_data # There are still duplicate rows from the join so I will drop them and keep only the first instance

merged_player_data = merged_player_data.drop_duplicates(subset=["Player"], keep='first').reset_index(drop=True)
merged_player_data # Now the merged dataframe is cleaned and finalized

"""# Load
Load data into AWS SQL Database
"""

!pip install mysql-connector-python
import mysql.connector

#connection information
mysql_address  = 'my-dataengineering-db.ctmou24cqxdc.us-east-2.rds.amazonaws.com'
mysql_username='ramonemartin'
mysql_password='355067-Dw'
mysql_database = 'my_dataengineering_db'

def get_conn_cur():
    cnx = mysql.connector.connect(user=mysql_username, password=mysql_password,
          host=mysql_address,
          database=mysql_database, port='3306');
    return (cnx, cnx.cursor())
def run_query(query_string):
  conn, cur = get_conn_cur() # get connection and cursor
  cur.execute(query_string) # executing string as before
  my_data = cur.fetchall() # fetch query data as before
  result_df = pd.DataFrame(my_data, columns=cur.column_names)
  cur.close() # close
  conn.close() # close
  return result_df


def sql_head(table_name):
  conn, cur = get_conn_cur() #get connection and cursor

  #get head information
  table_rows_query = """ SELECT * FROM %s LIMIT 5; """ % table_name
  cur.execute(table_rows_query)
  my_data = cur.fetchall() # fetch results

  # Create a dataframe that combines sql table with column names and return
  df = pd.DataFrame(my_data, columns=cur.column_names)

  cur.close()
  conn.close()
  return df

# put functions here and run
def get_table_names():
    conn, cur = get_conn_cur() # get connection and cursor

    # query to get table names from my_data_engineering_dbs schema
    table_name_query = """
    SELECT table_name FROM de-322-final-nba-stats.tables
    WHERE table_schema = 'my_dataengineering_dbs'; """

    # df = pd.read_sql(table_name_query, conn)
    cur.execute(table_name_query) # execute
    my_data = cur.fetchall() # fetch results


    # create a dataframe from the return data
    result_df = pd.DataFrame(my_data, columns=cur.column_names)

    cur.close() #close cursor
    conn.close() # close connection

    return  result_df

def get_column_names(table_name): # arguement of table_name
  conn, cur = get_conn_cur() # get connection and cursor

  # Now select column names while inserting the table name into the WERE
  column_name_query =  """SELECT column_name FROM information_schema.columns
       WHERE table_name = '%s' """ %table_name

  cur.execute(column_name_query) # exectue
  my_data = cur.fetchall() # store
  result_df = pd.DataFrame(my_data, columns=cur.column_names)

  cur.close() # close
  conn.close() # close

  return result_df # return

# Drop table
# *** Only run this code if you've already created your table and want to delete it to start from scratch again.
#
conn, cur = get_conn_cur()
table_name = 'player_draft_info' # what table to drop
drop_table_statement = "DROP TABLE %s;"%table_name # make your statement
cur.execute(drop_table_statement) # execute it
conn.commit()
cur.close() # close your cursor

# Drop table
# *** Only run this code if you've already created your table and want to delete it to start from scratch again.
#
conn, cur = get_conn_cur()
table_name = 'player_stats' # what table to drop
drop_table_statement = "DROP TABLE %s;"%table_name # make your statement
cur.execute(drop_table_statement) # execute it
conn.commit()
cur.close() # close your cursor

# Drop table
# *** Only run this code if you've already created your table and want to delete it to start from scratch again.
#
conn, cur = get_conn_cur()
table_name = 'player_facts' # what table to drop
drop_table_statement = "DROP TABLE %s;"%table_name # make your statement
cur.execute(drop_table_statement) # execute it
conn.commit()
cur.close() # close your cursor

conn, cur = get_conn_cur()

# Create player draft info table

tq = """CREATE TABLE player_draft_info (
  Player VARCHAR(255) NOT NULL PRIMARY KEY,
  draft_yr INTEGER,
  draft_rd INTEGER,
  draft_num INTEGER
);
"""

cur.execute(tq)

conn.commit()
conn.close()

sql_head(table_name='player_draft_info') # Checking to make sure it was successful

# Now I will load the data

iq = """INSERT INTO player_draft_info(Player, draft_yr, draft_rd, draft_num) VALUES(%s, %s, %s, %s); """
iq

draft_info = merged_player_data[['Player', 'Draft Yr', 'Draft Round', 'Draft Number']] # Making sub DF for SQL loading
# Convert pandas NA to None for SQL insertion
draft_info = draft_info.replace({pd.NA: None})

draft_info # Checking new DF

data_tups = [tuple(x) for x in draft_info.to_numpy()]

draft_info.to_numpy()

conn, cur = get_conn_cur()
cur.executemany(iq, data_tups)
conn.commit()
cur.close()
conn.close()

run_query("""SELECT * FROM player_draft_info LIMIT 5;""")

# Create player stats  table

tq = """CREATE TABLE player_stats (
  Player VARCHAR(255) NOT NULL PRIMARY KEY,
  PTS FLOAT,
  REB FLOAT,
  AST FLOAT,
  STL FLOAT,
  BLK FLOAT,
  PM FLOAT,
  MIN FLOAT,
  `FG_%` FLOAT,
  `3PT_%` FLOAT,
  `FT_%` FLOAT
);
"""

conn, cur = get_conn_cur()
cur.execute(tq)
conn.commit()
conn.close()

# Mini DF for SQL data loading
player_stats = merged_player_data[['Player', 'PTS', 'REB', 'AST', 'STL', 'BLK', 'PM', 'MIN', 'FG %', '3PT %', 'FT %']]
# Convert pandas NA to None for SQL insertion
player_stats = player_stats.replace({pd.NA: None})

# Now I will load the data

iq = """INSERT INTO player_stats(Player, PTS, REB, AST, STL, BLK, PM, MIN, `FG_%`, `3PT_%`, `FT_%`) VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s); """
iq

data_tups = [tuple(x) for x in player_stats.to_numpy()]

player_stats.to_numpy()

conn, cur = get_conn_cur()
cur.executemany(iq, data_tups)
conn.commit()
cur.close()
conn.close()

run_query("""SELECT * FROM player_stats LIMIT 5;""")

# Mini DF for SQL loading
player_facts = merged_player_data[['Player', 'Height', 'College', 'Country', 'Jersey Num']]
# Convert pandas NA to None for SQL insertion
player_facts = player_facts.replace({pd.NA: None})

# Create player facts  table

tq = """CREATE TABLE player_facts (
  Player VARCHAR(255) NOT NULL PRIMARY KEY,
  Height VARCHAR(255) NOT NULL,
  College VARCHAR(255) NOT NULL,
  Country VARCHAR(255) NOT NUll,
  Jersey_num INT NOT NULL
);
"""

conn, cur = get_conn_cur()
cur.execute(tq)
conn.commit()
conn.close()

#Load the data
iq = """INSERT INTO player_facts(Player, Height, College, Country, Jersey_num) VALUES(%s, %s, %s, %s, %s); """
iq

data_tups = [tuple(x) for x in player_facts.to_numpy()]

player_facts.to_numpy()

conn, cur = get_conn_cur()
cur.executemany(iq, data_tups)
conn.commit()
cur.close()
conn.close()

run_query("""SELECT * FROM player_facts LIMIT 5;""")

"""# Test Queries and Plots

I will run 5 tests

1) Top 10 players in PTS who went to UCLA
2) Top 10 players in Rebounds who are 6'3
3) Top 10 players in PTS drafted 1st overall
4) Top 10 players in PTS from the USA
5) Top 10 players in PTS with the jersey number 23
"""

run_query(""" SELECT t1.Player, PTS, College
FROM player_facts AS t1
INNER JOIN player_stats AS t2 ON t1.Player = t2.Player
WHERE College = 'UCLA'
ORDER BY t2.PTS DESC
LIMIT 10; """)

run_query(""" SELECT t1.Player, REB, Height
FROM player_facts AS t1
INNER JOIN player_stats AS t2 ON t1.Player = t2.Player
WHERE Height = '6-3'
ORDER BY t2.REB DESC
LIMIT 10; """)

run_query(""" SELECT t1.Player, PTS, draft_yr, draft_num
FROM player_draft_info AS t1
INNER JOIN player_stats AS t2 ON t1.Player = t2.Player
WHERE draft_num = 1
ORDER BY t2.PTS DESC
LIMIT 10; """)

run_query(""" SELECT t1.Player, PTS, Country
FROM player_facts AS t1
INNER JOIN player_stats AS t2 ON t1.Player = t2.Player
WHERE Country = 'USA'
ORDER BY t2.PTS DESC
LIMIT 10; """)

run_query(""" SELECT t1.Player, PTS, Jersey_num
FROM player_facts AS t1
JOIN player_stats AS t2 ON t1.Player = t2.Player
WHERE Jersey_num = 23
ORDER BY t2.PTS DESC
LIMIT 10; """)

"""# Plots

I will make 2 plots

1) A plot showing the avg scoring for players drafted by year drafted
2) PPG avg by country for the top 10 scoring countries
"""

import matplotlib.pyplot as plt
import seaborn as sns
avg_pts_by_draft_year = merged_player_data.groupby('Draft Yr')['PTS'].mean().reset_index()
avg_pts_by_draft_year = avg_pts_by_draft_year.dropna(subset=['Draft Yr'])
avg_pts_by_draft_year = avg_pts_by_draft_year.sort_values(by='Draft Yr')
sns.lineplot(x='Draft Yr', y='PTS', data=avg_pts_by_draft_year, marker='o')
plt.title('Average Points Per Game by Draft Year')
plt.xlabel('Draft Year')
plt.ylabel('Average PTS')
plt.grid(True)
plt.show()

avg_pts_by_country = merged_player_data.groupby('Country')['PTS'].mean().reset_index()
avg_pts_by_country = avg_pts_by_country.sort_values(by='PTS', ascending=False).head(10)
sns.barplot(x='Country', y='PTS', data=avg_pts_by_country, hue='Country', palette='viridis', legend=False)
plt.title('Average Points Per Game for Top 10 Scoring Countries')
plt.xlabel('Country')
plt.ylabel('Average PTS')
plt.xticks(rotation=45, ha='right')
plt.show()